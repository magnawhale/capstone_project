{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic use libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# for dashboard\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "# for scraping the web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from twitterscraper import query_tweets\n",
    "import twitterscraper\n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# file management\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "#for initial time-series modeling\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook Prophet libraries\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly, plot_cross_validation_metric\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "import plotly.offline as py\n",
    "pd.plotting.register_matplotlib_converters() #necessary to maintain pd.plotting functionality\n",
    "\n",
    "# for NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dates_list(year=2018):\n",
    "    # define a list of dates for a given year\n",
    "    dates_year =[str(date)[:10] for date in pd.date_range(start=f'1/1/{year}', end=f'12/31/{year}')]\n",
    "    # define a list of dates for generating file names\n",
    "    dates_stripped_year = [date.replace('-','') for date in dates_year]\n",
    "    return dates_year, dates_stripped_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to clean tweets and get tweet sentiment\n",
    "\n",
    "# portions of the code below comes from :\n",
    "# https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "def replace_emoticons(tweet):\n",
    "    \"This code replaces happy and sad emoticons with the words 'HAPPY' and 'SAD'\"\n",
    "    rhappy = '[' + re.escape(''.join(emoticons_happy)) + ']'\n",
    "    re.sub(rhappy, ' HAPPY ', tweet)\n",
    "    rsad = '[' + re.escape(''.join(emoticons_sad)) + ']'\n",
    "    re.sub(rsad, ' SAD ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet): \n",
    "    ''' \n",
    "    Utility function to clean tweet text by removing links, usernames, and\n",
    "    special characters using simple regex statements. \n",
    "    '''\n",
    "    tweet = replace_emoticons(tweet)\n",
    "    # p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION)\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
    "                            |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    return tweet\n",
    "\n",
    "def double_clean_tweet(tweet):\n",
    "    \"This function goes a little further than the previous clean function\"\n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', ' ', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', ' ', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet  (unless you want to later go through the UNICODE\n",
    "    # charts and separate \"happy\" emojis from \"sad\" emojis and add them to \n",
    "    # the `replace_emoticons()` function)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# Sentiment analysis code below adapted from:\n",
    "# https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(tweet)\n",
    "    # set sentiment \n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    if analysis.sentiment.polarity > 0.1: \n",
    "        sentiment = 'positive'\n",
    "    elif analysis.sentiment.polarity < -0.1: \n",
    "        sentiment = 'negative'\n",
    "    else: \n",
    "        sentiment = 'neutral'\n",
    "    return sentiment, polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create a .CSV file that compiles the relevant \n",
    "# info from the JSONs, preprocesses the tweets, and performs sentiment analysis\n",
    "def json_to_csv_tweets(output_filename='output.csv', year=2018):\n",
    "    \"\"\"\n",
    "    Takes in JSON files of scraped tweets from the `./data/` folder,\n",
    "    cleans the tweets, performs sentiment analysis, and then outputs\n",
    "    the results to the provided destination CSV filename.\n",
    "    \"\"\"\n",
    "    # create the csv writer object\n",
    "    csvwriter = csv.writer(open(output_filename, 'w', newline=''))\n",
    "    csvwriter.writerow([\"timestamp\", \"text\", \"sentiment\", \"polarity\", \"subjectivity\", \"tally\"])\n",
    "\n",
    "    # iterate adding rows of JSON to the CSV file\n",
    "    dates_year, dates_stripped_year = make_dates_list(year)\n",
    "    for i in dates_stripped_year:\n",
    "        f = open(f'./data/t{i}.json')\n",
    "        data = json.load(f)\n",
    "        for tweet in data:\n",
    "            tw = tweet[\"text\"]\n",
    "            tw = replace_emoticons(tw)\n",
    "            tw = clean_tweet(tw)\n",
    "            tw = double_clean_tweet(tw)\n",
    "            sentiment, polarity, subjectivity = get_tweet_sentiment(tw)\n",
    "            csvwriter.writerow([i, tw, sentiment, polarity, subjectivity, 1])\n",
    "        f.close()\n",
    "        if float(i)%20 == 0:\n",
    "            print(f\"Finished working with:   ./data/t{i}.json\")\n",
    "    print(\"JOB IS COMPLETELY FINISHED.  HOORAY!!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to scrape and clean financial data for a given year\n",
    "def fetch_data(symbol, object_type, stocks_apikey):\n",
    "    # Making an API request for a certain stock's history\n",
    "    if object_type == 'currency':\n",
    "        credentials = {'function':'FX_DAILY', \n",
    "                       'from_symbol':symbol, \n",
    "                       'to_symbol':'USD', \n",
    "                       'outputsize':'full',\n",
    "                       'apikey':stocks_API_key}\n",
    "    elif object_type == 'cryptocurrency':\n",
    "        credentials = {'function':'DIGITAL_CURRENCY_DAILY', \n",
    "                       'symbol':symbol, \n",
    "                       'market':'USD', \n",
    "                       'apikey':stocks_API_key}\n",
    "    else:\n",
    "        credentials = {'function':'TIME_SERIES_DAILY',\n",
    "                       'symbol':symbol,\n",
    "                       'outputsize':'full',\n",
    "                       'apikey':stocks_apikey}\n",
    "\n",
    "    r = requests.get('https://www.alphavantage.co/query', params=credentials)\n",
    "    # checking to make sure request was successful\n",
    "    print(r.status_code)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print(\"Request Successful\")\n",
    "    return r\n",
    "\n",
    "\n",
    "def clean_financials(r, object_type):\n",
    "    # cleaning up the data to make it easier to work with\n",
    "    if object_type == 'currency':\n",
    "        df = pd.DataFrame(r.json()[\"Time Series FX (Daily)\"])\n",
    "    elif object_type == 'cryptocurrency':\n",
    "        df = pd.DataFrame(r.json()[\"Time Series (Digital Currency Daily)\"])\n",
    "    elif object_type in ['stock','index']:\n",
    "        df = pd.DataFrame(r.json()[\"Time Series (Daily)\"])\n",
    "\n",
    "    df = df.T.reset_index()\n",
    "\n",
    "    if object_type == 'cryptocurrency':\n",
    "        df.drop(columns=[\"1b. open (USD)\",\"2b. high (USD)\",\"3b. low (USD)\",\n",
    "            \"4b. close (USD)\",\"6. market cap (USD)\"], inplace=True, axis=1)\n",
    "\n",
    "    if object_type == 'currency':\n",
    "        df.columns = ['date','open','high','low','close']\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        df[['open','high','low','close']] = df[['open',\n",
    "            'high','low','close']].astype(float)\n",
    "    else:\n",
    "        df.columns = ['date','open','high','low','close','volume']\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        df[['open','high','low','close','volume']] = df[['open',\n",
    "            'high','low','close','volume']].astype(float)\n",
    "\n",
    "    # create a new column to account for after-hours trading\n",
    "    # this uses the next day's open value as the prior day's close value\n",
    "    cl24 = [df.loc[0].close]\n",
    "    for val in df.open.values:\n",
    "        cl24.append(val)\n",
    "    cl24 = pd.DataFrame(cl24[:-1], columns=['close_24'])\n",
    "    df = df.join(cl24)\n",
    "\n",
    "    # now we must account for when afterhours trading exceeds high/low values\n",
    "    df['high_24'] = df[['high', 'close_24']].values.max(1)\n",
    "    df['low_24'] = df[['low', 'close_24']].values.min(1)\n",
    "    # and add a few more columns that should be useful\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['range_24'] = df['high_24'] - df['low_24']\n",
    "    df['change_24'] = df['close_24'] - df['open']\n",
    "    # setting date column as index to facilitate timeseries manipulation\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def get_financial_data(symbol, object_type, stocks_apikey, year=2018, verbose=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    symbol         (string) Stock or Currency symbol\n",
    "    object_type    (string) must be one of these:\n",
    "                      'stock'\n",
    "                      'index'\n",
    "                      'currency'\n",
    "                      'cryptocurrency'\n",
    "    stocks_apikey  (string) your API key for \n",
    "                      https://www.alphavantage.co\n",
    "    year           (int) the year you wish to examine\n",
    "    verbose        if True, displays .info() and .head() of data\n",
    "    =========================================\n",
    "    Returns a DataFrame of daily financial information containing\n",
    "    at least opening, closing, high, and low values.\n",
    "    \"\"\"\n",
    "    valid_types = ['stock','index','currency','cryptocurrency']\n",
    "    if object_type in valid_types:\n",
    "        r = fetch_data(symbol, object_type, stocks_apikey)\n",
    "        df = clean_financials(r, object_type)    #cleaning data\n",
    "        year_df = df[f'{year}':f'{year}']        # getting 1 year's data\n",
    "        if verbose:\n",
    "            display(year_df.head(),year_df.info())\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(year_df['close_24'])\n",
    "        plt.title(f\"{symbol} Daily Performance for {year}\", fontsize=16)\n",
    "        plt.ylabel(\"Price (in USD)\");\n",
    "        return year_df\n",
    "    else:\n",
    "        print(\"\"\"\n",
    "        Invalid entry for 'object_type', must be one of these strings:\n",
    "                'stock'\n",
    "                'index'\n",
    "                'currency'\n",
    "                'cryptocurrency'\n",
    "        \"\"\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to decompose a time series, in order to detect\n",
    "# trends and seasonality, and allow for examining the residuals\n",
    "def df_decompose(df):\n",
    "    # Gather the trend, seasonality and noise of decomposed object\n",
    "    trend = sd(df).trend\n",
    "    seasonal = sd(df).seasonal\n",
    "    residual = sd(df).resid\n",
    "\n",
    "    # Plot gathered statistics\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(f\"Decomposition for {df}\")\n",
    "    plt.subplot(411)\n",
    "    plt.plot(df, label='Original', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal, label='Seasonality', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residuals', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    return residual.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot sentiment trends\n",
    "def plot_sentiments(csv_filename):\n",
    "    year=2018\n",
    "    tweets_df = pd.read_csv(csv_filename)\n",
    "    tweets_df.timestamp = pd.to_datetime(tweets_df.timestamp, format='%Y%m%d')\n",
    "    grouped = pd.DataFrame(tweets_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()\n",
    "    for sentiment in grouped.sentiment.unique():\n",
    "        temp_df = grouped[grouped.sentiment == sentiment].set_index('timestamp')\n",
    "        temp_df['tally'].plot(figsize=(15,8), label=sentiment)\n",
    "        plt.ylabel(\"Number of Tweets by Sentiment\\n(~1,000/day total)\", fontsize=16)\n",
    "        plt.title(f\"Daily Sentiment at midnight (UTC) in {year}\", fontsize=20)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to automate the process of inspecting lunar trends\n",
    "\n",
    "def get_lunar_phases(year='2018'):\n",
    "    \"Given a year, returns a dataframe of lunar phases, dates, and times (UTC).\"\n",
    "    url = f\"https://aa.usno.navy.mil/cgi-bin/aa_phases.pl?year={year}&nump=65&format=t\"\n",
    "    res_page = requests.get(url)\n",
    "    soup = BeautifulSoup(res_page.content, 'html.parser')\n",
    "    table_cells = soup.find_all(\"td\")\n",
    "    output = pd.DataFrame(columns=['phase','date','time'])\n",
    "    for i in range(len(table_cells)):\n",
    "        row = np.floor(i/2)\n",
    "        if i%2 == 0:\n",
    "            output.at[row,'phase'] = table_cells[i].text\n",
    "        else:\n",
    "            output.at[row,'date'] = table_cells[i].text[:12]  #need to grab just beggining of string\n",
    "            output.at[row,'time'] = table_cells[i].text[-5:]  #need to grab just ending of string\n",
    "    output.date = pd.to_datetime(output.date)\n",
    "    output.reset_index(drop=True, inplace=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "def lunar_phase_separator(phases_df, lower_window=0, upper_window=1):\n",
    "    \"\"\"\n",
    "    Converts DataFrame of moon phases into FBProphet-friendly format.\n",
    "    ---------------------\n",
    "    Inputs:\n",
    "    phases_df       DataFrame containing lunar phase dates for a given year\n",
    "                        (the output of the `get_lunar_phases()` function)\n",
    "    lower_window    (<=0) number of days prior to moon phase to include in 'holiday'\n",
    "    upper_window    (>=0) number of days after to moon phase to include in 'holiday'\n",
    "    ---------------------\n",
    "    Returns:        FBProphet-friendly DataFrame for use in 'holiday' parameter\n",
    "    \"\"\"\n",
    "    # let's separate the different moon phases\n",
    "    phase_names = ['Full Moon','Last Quarter','New Moon','First Quarter']\n",
    "    ph_list = []\n",
    "    for phase in phase_names:\n",
    "        moons = pd.DataFrame(phases_df.loc[phases_df['phase'] == phase]['date']).reset_index(drop=True)\n",
    "        moons.columns = ['ds']\n",
    "        moons['holiday'] = str(phase).lower().replace(\" \", \"\")\n",
    "        moons['lower_window'] = lower_window\n",
    "        moons['upper_window'] = upper_window\n",
    "        ph_list.append(moons)\n",
    "    phases = pd.concat((ph_list[0], ph_list[1], ph_list[2], ph_list[3]))\n",
    "    return phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to automate the process of inspecting lunar trends\n",
    "def prep_data_for_FBP(data, column_name):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and the name of the column to be processed, \n",
    "    generates a FBProphet-ready DataFrame.\n",
    "    \"\"\"\n",
    "    d = data.reset_index()\n",
    "    prepped_data = d[['date', column_name]].sort_values(by=['date']).reset_index(drop=True)\n",
    "    prepped_data.columns = ['ds','y']\n",
    "    return prepped_data\n",
    "\n",
    "\n",
    "def cross_val_FBP(model, metric='rmse', show_metric_scores=False):\n",
    "    # cross validating using time horizons within the dataset\n",
    "    df_cv = cross_validation(model, initial='90 days', period='15 days', horizon = '30 days')\n",
    "    # performance metrics for the FBProphet model\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    if show_metric_scores == True:\n",
    "        display(df_p.head())\n",
    "    # plotting performance metrics\n",
    "    fig = plot_cross_validation_metric(df_cv, metric)\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_weekends(year='2018'):\n",
    "    weekends_df = pd.DataFrame(columns=['date','day_of_week'])\n",
    "    weekends_df.date = [date for date in pd.date_range(start=f'1/1/{year}', periods=470)]\n",
    "    weekends_df.day_of_week = [datetime.datetime(int(str(date)[:4]), \n",
    "        int(str(date)[5:7]), int(str(date)[8:10])).weekday() for date in weekends_df.date]\n",
    "    weekends_df = weekends_df[weekends_df.day_of_week >= 5].reset_index(drop=True)\n",
    "    return weekends_df\n",
    "\n",
    "\n",
    "def lunar_stock_trend(df, column_name, phases_df, year='2018', lower_window=0, \n",
    "                      upper_window=1, trades_on_weekends=False,\n",
    "                      cross_val=True, metric='rmse',\n",
    "                      show_metric_scores=False):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame of stock data and the \n",
    "    column name for the feature to be examined, a DataFrame of lunar \n",
    "    phases for a year, the desired year, and the lower & upper \n",
    "    windows for 'holiday' dates.\n",
    "    \n",
    "    If the financial data contains values for weekends, set the\n",
    "    'trades_on_weekends' parameter to 'True'.\n",
    "    \n",
    "    Additional option to cross-validate FBProphet model predictions, and\n",
    "    select from a variety of metrics to use.\n",
    "    \n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    Returns a list containing the model object, the 'future' dataframe\n",
    "    used to make predictions, the forecast output DataFrame, a graph\n",
    "    of the forecast, and a graph of forecast components.\n",
    "    \"\"\"\n",
    "    phases = lunar_phase_separator(phases_df, lower_window, upper_window)\n",
    "    if trades_on_weekends == False:\n",
    "        weekends_df = get_weekends(year)\n",
    "        phases = phases[~phases['ds'].isin(weekends_df.date)]\n",
    "    data = prep_data_for_FBP(df, column_name)\n",
    "    m = Prophet(holidays=phases)\n",
    "    m.fit(data)\n",
    "    future = m.make_future_dataframe(periods=60, freq='D')\n",
    "    if trades_on_weekends == False:\n",
    "        future = future[~future['ds'].isin(weekends_df.date)]\n",
    "    forecast = m.predict(future)\n",
    "    disp_length = 4 * (1 + abs(lower_window) + abs(upper_window))\n",
    "    display(forecast[(forecast['fullmoon'] + forecast['lastquarter'] + \n",
    "                      forecast['newmoon'] + forecast['firstquarter']).abs() > \n",
    "                     0][['ds', 'fullmoon', 'lastquarter', 'newmoon',\n",
    "                         'firstquarter']][:disp_length])        \n",
    "    fig1 = m.plot(forecast);\n",
    "    fig1.set_size_inches(15, 5);\n",
    "    fig2 = m.plot_components(forecast)\n",
    "    fig2.set_size_inches(15, 10);\n",
    "    if cross_val:\n",
    "        cross_val_FBP(m, metric, show_metric_scores)\n",
    "        \n",
    "    return [m, future, forecast, fig1, fig2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_fbprophet(filename='tweets_nowords_2018.csv'):\n",
    "    print(filename)\n",
    "    tweets_df = pd.read_csv(filename)\n",
    "    tweets_df.timestamp = pd.to_datetime(tw_df.timestamp, format='%Y%m%d')\n",
    "    grouped = pd.DataFrame(tweets_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()\n",
    "\n",
    "    #prepare grouped sentiment data for FBProphet processing\n",
    "    #here, positive sentiment only\n",
    "    grp_pos = grouped[grouped.sentiment == 'positive'].drop('sentiment', axis=1).reset_index(drop=True)\n",
    "    grp_pos.columns = ['ds','y']\n",
    "\n",
    "    m = Prophet(holidays=phases)\n",
    "    m.fit(grp_pos)\n",
    "    future = m.make_future_dataframe(periods=60, freq='D')\n",
    "    forecast = m.predict(future)\n",
    "    forecast[(forecast['fullmoon'] + forecast['lastquarter'] + \n",
    "              forecast['newmoon'] + forecast['firstquarter']).abs() > \n",
    "             0][['ds', 'fullmoon', 'lastquarter', 'newmoon','firstquarter']][:10]\n",
    "    fig1 = m.plot(forecast);\n",
    "    fig1.set_size_inches(15, 5);\n",
    "    fig2 = m.plot_components(forecast)\n",
    "    fig2.set_size_inches(15, 10);\n",
    "    \n",
    "    # cross validating using time horizons within the dataset\n",
    "    df_cv = cross_validation(m, initial='90 days', period='15 days', horizon = '30 days')\n",
    "#     # performance metrics for the FBProphet model\n",
    "#     df_p = performance_metrics(df_cv)\n",
    "#     display(df_p.head())\n",
    "\n",
    "    # plotting performance metrics\n",
    "    fig3 = plot_cross_validation_metric(df_cv, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get most frequent words\n",
    "def get_top_n_words(corpus, stopwords):\n",
    "    vec = CountVectorizer(stop_words=stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq\n",
    "\n",
    "\n",
    "# function for plotting the most frequent words\n",
    "def plot_phase_words(corpus, keywords, phase, stopwords, n=None):\n",
    "    comm_words = get_top_n_words(corpus.text, stopwords)[:n]\n",
    "    df2 = pd.DataFrame(comm_words, columns=['text', 'count'])\n",
    "    total = df2['count'].sum()   ###################    \n",
    "    # plotting\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    plt.xticks(rotation=30, fontsize=14)\n",
    "    hghts = ((df2['count'] / total) * 100)\n",
    "    rects = ax.bar(df2.text, hghts)                                                  #################\n",
    "    ax.set_title(f\"Top {n} Words for {phase} at Midnight UTC\", fontsize=16)\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width(), 1.005*height,\n",
    "                f'{np.round(height, 2)}%', ha='right', va='bottom')\n",
    "    plt.plot([], [], ' ', label=f\"Search Keywords:\\n{keywords}\")\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "# function to parse a CSV file and then plot all results\n",
    "def plot_CSV_words(filename, keywords, phases_df, n=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    filename       (str) CSV filename generated from Twitter scraping function\n",
    "    keywords       (str) the query keywords used in Twitter scraping function\n",
    "    phases_df      (df) dataframe of lunar phases for a given year\n",
    "    n              (int) number of most common words to return\n",
    "    ==================================================\n",
    "    Returns:\n",
    "    Histograms (5 total) of the 'n' most common words for each lunar phase and for\n",
    "    all the nights not in a lunar phase.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df.dropna(inplace=True)\n",
    "    df.timestamp = pd.to_datetime(df.timestamp, format='%Y%m%d')  \n",
    "    stopwords = set(ENGLISH_STOP_WORDS)\n",
    "    # now add extra things to that list that we want to filter out\n",
    "    stopwords.update(['twitter','com','pic','ve','ll','just','like','don','really','00'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    phase_list = ['Full Moon','Last Quarter','New Moon','First Quarter','No Phase']\n",
    "    for phase in phase_list:\n",
    "        all_moons = list(phases_df.date.astype(str))\n",
    "        if phase != 'No Phase':\n",
    "            moon = list(phases_df[phases_df.phase == phase].date.astype(str))\n",
    "            moon_data = df.loc[df.index.floor('D').isin(moon)]\n",
    "            plot_phase_words(moon_data, keywords, phase, stopwords, n=n)\n",
    "        else:\n",
    "            moon_data = df.loc[~df.index.floor('D').isin(all_moons)]\n",
    "            plot_phase_words(moon_data, keywords, phase, stopwords, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that has to be executed\n",
    "\n",
    "reading files and making dataframes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_2018_df = get_lunar_phases(year='2018')\n",
    "phases = lunar_phase_separator(phases_2018_df, lower_window=0, upper_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df = pd.read_csv('tweets_happysad_2018.csv')\n",
    "tw_df.timestamp = pd.to_datetime(tw_df.timestamp, format='%Y%m%d')\n",
    "grouped = pd.DataFrame(tw_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment in grouped.sentiment.unique():\n",
    "#     print(sentiment)\n",
    "#     df_decompose(t2018_log[sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiments('tweets_happysad_2018.csv', 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_API_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT_2018 = get_financial_data('MSFT', 'stock', stocks_API_key, year=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_stock_trend(df=MSFT_2018,\n",
    "                  column_name='change_24',\n",
    "                  phases_df=phases_2018_df,\n",
    "                  year='2018',\n",
    "                  lower_window=-1,\n",
    "                  upper_window=1,\n",
    "                  trades_on_weekends=False,\n",
    "                  cross_val=True,\n",
    "                  metric='rmse', \n",
    "                  show_metric_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_csv_files = ['tweets_lovehate_2018.csv',\n",
    "                   'tweets_happysad_2018.csv',\n",
    "                   'tweets_music_2018.csv',\n",
    "                   'tweets_money_2018.csv',\n",
    "                   'tweets_nowords_2018.csv',\n",
    "                   'tweets_politics_2018.csv',\n",
    "                   'tweets_coding_2018.csv']\n",
    "\n",
    "queries = ['love OR peace OR hate OR war',\n",
    "           'happy OR sad OR life OR death',\n",
    "           'music OR tunes OR dance',\n",
    "           'stocks OR money OR taxes',\n",
    "           '(no keywords entered)',\n",
    "           'politics OR government OR Trump',\n",
    "           \"'data science' OR coding OR programming\"] \n",
    "\n",
    "for filename, key_words in zip(tweet_csv_files, queries):\n",
    "    plot_CSV_words(filename=filename,\n",
    "                   keywords=key_words,\n",
    "                   phases_df=phases_2018_df,\n",
    "                   n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring daily positivity rates by query phrase....aggregated\n",
    "for search in tweet_csv_files:\n",
    "    plot_sentiments(search, 2018, positive_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing graphs for each query separately\n",
    "for search in tweet_csv_files:\n",
    "    plot_sentiments(csv_filename=search, year=2018, positive_only=False)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running each CSV file through FBPophet\n",
    "for search in tweet_csv_files:\n",
    "    tweet_fbprophet(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dash section\n",
    "\n",
    "some of the websites with info:\n",
    "   * Intro to Dash (blog) https://medium.com/plotly/introducing-dash-5ecf7191b503\n",
    "   * Interactive Dashboards w Dash (blog) https://alysivji.github.io/reactive-dashboards-with-dash.html\n",
    "   * Dash GitHub https://github.com/plotly/dash\n",
    "   * Dash documentation https://dash.plot.ly/?_ga=2.22784251.1143889031.1570652152-637402008.1568664543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing .CSV data for use in Panel\n",
    "quers = ['lovehate','happysad','music','money','nowords','politics', 'coding']\n",
    "symbols = ['MSFT','EUR','BTC','SP500','SBAC']\n",
    "\n",
    "\n",
    "lovehate_df = pd.read_csv('tw_lovehate.csv')\n",
    "lovehate_df.timestamp = pd.to_datetime(lovehate_df.timestamp, infer_datetime_format=True)\n",
    "happysad_df = pd.read_csv('tw_happysad.csv')\n",
    "happysad_df.timestamp = pd.to_datetime(happysad_df.timestamp, infer_datetime_format=True)\n",
    "music_df = pd.read_csv('tw_music.csv')\n",
    "music_df.timestamp = pd.to_datetime(music_df.timestamp, infer_datetime_format=True)\n",
    "money_df = pd.read_csv('tw_money.csv')\n",
    "money_df.timestamp = pd.to_datetime(money_df.timestamp, infer_datetime_format=True)\n",
    "nowords_df = pd.read_csv('tw_nowords.csv')\n",
    "nowords_df.timestamp = pd.to_datetime(nowords_df.timestamp, infer_datetime_format=True)\n",
    "politics_df = pd.read_csv('tw_politics.csv')\n",
    "politics_df.timestamp = pd.to_datetime(politics_df.timestamp, infer_datetime_format=True)\n",
    "coding_df = pd.read_csv('tw_coding.csv')\n",
    "coding_df.timestamp = pd.to_datetime(coding_df.timestamp, infer_datetime_format=True)\n",
    "\n",
    "MSFT_df = pd.read_csv('MSFT.csv')\n",
    "MSFT_df.date = pd.to_datetime(MSFT_df.date, infer_datetime_format=True)\n",
    "EUR_df = pd.read_csv('EUR.csv')\n",
    "EUR_df.date = pd.to_datetime(EUR_df.date, infer_datetime_format=True)\n",
    "BTC_df = pd.read_csv('BTC.csv')\n",
    "BTC_df.date = pd.to_datetime(BTC_df.date, infer_datetime_format=True)\n",
    "SP500_df = pd.read_csv('SP500.csv')\n",
    "SP500_df.date = pd.to_datetime(SP500_df.date, infer_datetime_format=True)\n",
    "SBAC_df = pd.read_csv('SBAC.csv')\n",
    "SBAC_df.date = pd.to_datetime(SBAC_df.date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dash==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install dash-daq==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "### This cell must be running in order to test whether ###\n",
    "###  the code below which has been saved as `app.py`   ###\n",
    "###   is properly functioning when output as HTML.     ###\n",
    "##########################################################\n",
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "##### loading in the information we'll be displaying #####\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "#### BELOW IS THE CODE FOR THE DASHBOARD ####\n",
    "#############################################\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "tw_df = pd.read_csv('tw_sent.csv')\n",
    "tw_df.date = pd.to_datetime(tw_df.date, infer_datetime_format=True)\n",
    "\n",
    "moon_df = pd.read_csv('phases.csv')\n",
    "moon_df.date = pd.to_datetime(moon_df.date, infer_datetime_format=True)\n",
    "\n",
    "queries = ['love OR peace OR hate OR war',\n",
    "           'happy OR sad OR life OR death',\n",
    "           'music OR tunes OR dance',\n",
    "           'stocks OR money OR taxes',\n",
    "           '(no keywords entered)',\n",
    "           'politics OR government OR Trump',\n",
    "           'data science OR coding OR programming']\n",
    "moons = ['Full Moon','Last Quarter','New Moon','First Quarter']\n",
    "\n",
    "\n",
    "####Application layout\n",
    "app.layout = html.Div(children=[\n",
    "   #Giving the page a title/header \n",
    "    html.H1(\n",
    "        children='Lunar Cycles & Human Behavior',\n",
    "        style={'textAlign': 'center'}\n",
    "    ),\n",
    "    \n",
    "    # The area with the dropdown menus\n",
    "    html.Div([\n",
    "        \n",
    "        # Adding a dropdown menu\n",
    "        html.Div([\n",
    "            html.Label('Twitter Search Phrases:'),\n",
    "            dcc.Dropdown(\n",
    "                id='query-dropdown',\n",
    "                options=[{'label': i, 'value': i} for i in queries],\n",
    "                #value='MTL'       # default initial value...remove to default as blank\n",
    "            ),\n",
    "        ]),\n",
    "        \n",
    "        # Adding a second dropdown menu\n",
    "        html.Div([\n",
    "            html.Label('Moon Phrase:'),\n",
    "            dcc.Dropdown(\n",
    "                id='moon-dropdown',\n",
    "                options=[{'label': i, 'value': i} for i in moons],\n",
    "                #value='MTL'       # default initial value...remove to default as blank\n",
    "            ),\n",
    "        ]),\n",
    "        \n",
    "        # setting the layout of the dropdown DIV area\n",
    "        #style={'width': '30%', 'float': 'right', 'display': 'inline-block'}  ###need to experiment with these\n",
    "    ]),\n",
    "    \n",
    "    #The area with the display\n",
    "    dcc.Graph(id='tw-sent-graph')\n",
    "\n",
    "    \n",
    "    \n",
    "#     # TABS\n",
    "#     dcc.Tabs(id='tabs', value='tab-1', children=[\n",
    "#         dcc.Tab(id='tab1', label='Daily Sentiment', value='tab-1'),\n",
    "#         dcc.Tab(id='tab2', label='Word Frequencies', value='tab-2'),\n",
    "#     ]),\n",
    "#     html.Div(id='tabs-content')\n",
    "    \n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "##### Callbacks section for linking everything together #####\n",
    "\n",
    "# @app.callback(Output('tabs-content', 'children'),\n",
    "#               [Input('tabs', 'value')])\n",
    "# def render_content(tab):\n",
    "#     if tab == 'tab-1':\n",
    "#         return html.Div([\n",
    "#             html.H3('Tab content 1')    ### Whatever you want to display here\n",
    "#         ])\n",
    "#     elif tab == 'tab-2':\n",
    "#         return html.Div([\n",
    "#             html.H3('Tab content 2')    ### Whatever you want to display here\n",
    "#         ])\n",
    "\n",
    "@app.callback(\n",
    "    Output('tw-sent-graph', 'figure'),\n",
    "    [Input('query-dropdown', 'value'),\n",
    "     Input('moon-dropdown', 'value')])\n",
    "def update_graph(selected_query, selected_moon):\n",
    "    tweets_df = tw_df[tw_df['query'] == selected_query]\n",
    "    grouped = pd.DataFrame(tweets_df.groupby(['date', 'sentiment'])['tally'].sum()).reset_index()\n",
    "    traces = []\n",
    "    for sentiment in grouped.sentiment.unique():\n",
    "        temp_df = grouped[grouped.sentiment == sentiment]\n",
    "        traces.append(go.Scatter(\n",
    "                            x=temp_df.date,\n",
    "                            y=temp_df['tally'],\n",
    "                            name=sentiment,\n",
    "                            text=temp_df['sentiment'],\n",
    "                            mode='lines',\n",
    "                            opacity=0.8))\n",
    "\n",
    "    figure = {'data': traces,\n",
    "        'layout': go.Layout(colorway=[\"#5E0DAC\", '#FF4F00', '#375CB1', '#FF7400', '#FFF400', '#FF0056'],\n",
    "#                            height=600,\n",
    "                            title=f\"Daily Sentiment at Midnight (UTC) for : '{selected_query}'\",\n",
    "                            xaxis={\"title\":\"Date\",\n",
    "                                   'rangeslider': {'visible': True},\n",
    "                                   'type': 'date'},\n",
    "                            yaxis={\"title\":\"Sentiment Quantity (~1,000/day total)\"})}\n",
    "    return figure\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# automatically update HTML display if a change is made to code\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-capstone-env] *",
   "language": "python",
   "name": "conda-env-.conda-capstone-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
